{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "generate train data, validation data, and test data\n",
    "\"\"\"\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import imread\n",
    "from scipy.misc import imsave\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import casia_b_LSTM_identification_helper\n",
    "from tensorflow_vgg import vgg16\n",
    "from tensorflow_vgg import utils\n",
    "import collections\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gallery length: (124,)\n",
      "gallery shape: (53, 1000)\n",
      "\n",
      "probe length: (124,)\n",
      "probe shape: (51, 1000)\n"
     ]
    }
   ],
   "source": [
    "# get codes\n",
    "gallery_features_batch1 = np.load(open(r'gait_data/casia_b_fc8_124/casia_b_72c_124_seq1', mode='rb'))\n",
    "probe_features_batch1 = np.load(open(r'gait_data/casia_b_fc8_124/casia_b_72c_124_seq2', mode='rb'))\n",
    "# feature shape\n",
    "print(\"gallery length: {}\".format(gallery_features_batch1.shape))\n",
    "print(\"gallery shape: {}\".format(gallery_features_batch1[0].shape))\n",
    "print(\"\")\n",
    "print(\"probe length: {}\".format(probe_features_batch1.shape))\n",
    "print(\"probe shape: {}\".format(probe_features_batch1[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gallery_features_batch1_add0 = np.zeros([len(gallery_features_batch1), 82, 1000], dtype=float)\n",
    "probe_features_batch1_add0 = np.zeros([len(probe_features_batch1), 82, 1000], dtype=float)\n",
    "\n",
    "g_seq_length = []\n",
    "p_seq_length = []\n",
    "for ii, (gallery, probe) in enumerate(zip(gallery_features_batch1, probe_features_batch1)):\n",
    "    gallery_length = gallery.shape[0]\n",
    "    probe_length = probe.shape[0]\n",
    "    g_seq_length.append(gallery_length)\n",
    "    p_seq_length.append(probe_length)\n",
    "    \n",
    "    gallery_features_batch1_add0[ii, :gallery_length, :] = gallery\n",
    "    probe_features_batch1_add0[ii, :probe_length, :] = probe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target 1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_1_target_1 = np.concatenate((gallery_features_batch1_add0, probe_features_batch1_add0), axis=1)\n",
    "target_1 = np.ones([batch_1_target_1.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_1_target_1: (124, 164, 1000)\n",
      "\n",
      "target_1: (124,)\n"
     ]
    }
   ],
   "source": [
    "print(\"batch_1_target_1:\", batch_1_target_1.shape)\n",
    "print(\"\")\n",
    "print(\"target_1:\", target_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target 0 Data\n",
    "We create a same length batch with shuffled data\n",
    "直到检测shuffle的idx没有重叠，函数会返回ok，否则打印error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 34, 116,  61,  94,  69,   3,  59, 113,  19,  31, 110,  22,  93,\n",
       "        96,  27,  14,  55,  54,  75, 123,  74, 100,  15,  57,  47,  44,\n",
       "        45,  48,  60,   2,  39,   0,  46,  17,  58,  30,  82, 121,  79,\n",
       "        88,   9,  87,  84, 103,  42,  35, 111,  71,  18,  41,  38, 109,\n",
       "        95,  65,   1,  64, 115,  62,  23,  89,  53,  25,  29, 105,  21,\n",
       "        50, 117,  13,  83,  12, 122,  72,  92, 120,   7,  20,  85, 101,\n",
       "        26,  66,  49,  98,   6,  97,  32,  10,  81,  91,  33,  40,  77,\n",
       "        43,  68,  52, 114,  73, 112,  51,  63,  78,  67,  99,  70,  16,\n",
       "       102,  24,  36, 104,  76,  86, 107,  90, 106,  37,   4,  56,  11,\n",
       "         5, 119,  28,   8,  80, 118, 108])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.arange(len(gallery_features_batch1_add0))\n",
    "np.random.shuffle(idx)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "def exam_shuffle_idx(idx):\n",
    "    x =None\n",
    "    for ii, index in enumerate(idx):\n",
    "        if ii == index:\n",
    "            x ='error'\n",
    "            print(x)\n",
    "            break\n",
    "    if x != 'error':\n",
    "        print(\"ok\")\n",
    "    return None\n",
    "exam_shuffle_idx(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 82, 1000)\n"
     ]
    }
   ],
   "source": [
    "# create shuffled probe_features_batch1\n",
    "shuffled_probe_features_batch1 = np.zeros(probe_features_batch1_add0.shape)\n",
    "print(shuffled_probe_features_batch1.shape)\n",
    "for ii, index in enumerate(idx):\n",
    "    shuffled_probe_features_batch1[ii, :, :] = probe_features_batch1_add0[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_1_target_0 = np.concatenate((gallery_features_batch1_add0, shuffled_probe_features_batch1), axis=1)\n",
    "target_0 = np.zeros([batch_1_target_0.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2820414 ,  0.31092837,  0.30656928, ...,  0.16087618,\n",
       "         0.31453818,  0.39509431],\n",
       "       [ 0.27821377,  0.28756857,  0.3085832 , ...,  0.1579583 ,\n",
       "         0.29957786,  0.40155482],\n",
       "       [ 0.29017514,  0.30860469,  0.32465994, ...,  0.15748253,\n",
       "         0.28548372,  0.39662218],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_target_0[1][82:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2820414 ,  0.31092837,  0.30656928, ...,  0.16087618,\n",
       "         0.31453818,  0.39509431],\n",
       "       [ 0.27821377,  0.28756857,  0.3085832 , ...,  0.1579583 ,\n",
       "         0.29957786,  0.40155482],\n",
       "       [ 0.29017514,  0.30860469,  0.32465994, ...,  0.15748253,\n",
       "         0.28548372,  0.39662218],\n",
       "       ..., \n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1_target_1[116][82:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248, 164, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate half_batch_1_target_0 and half_batch_1_target_1 together\n",
    "new_batch_1 = np.concatenate((batch_1_target_1, batch_1_target_0), axis=0)\n",
    "new_batch_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gait_data/casia_b_fc8_124/ordered_new_batch_1', 'wb') as f:\n",
    "    np.save(f, new_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_1_and_0 = np.concatenate((target_1, target_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([113,  47, 104,   3,  59,  83,  78,  27,  56,   4, 191, 167, 207,\n",
       "       214,  13, 166,  60,  57, 105,  49, 149, 209, 162,   0,  75,  64,\n",
       "        84, 152, 129, 230,  16, 114, 238, 164, 192,  81, 201, 130, 234,\n",
       "       116,  71, 115, 241,  15, 153, 198, 102, 123,  11,  14, 210, 227,\n",
       "       106, 242, 160, 237, 195, 150,  92, 193, 165, 240, 184, 141,  39,\n",
       "       177, 219, 127, 233,  30, 148, 188, 228,  61, 243, 244, 181, 118,\n",
       "       204,  18,  48, 236, 111,  52,  24,  45, 187,  99, 155, 208, 205,\n",
       "       235, 189, 203, 157,  85, 218, 156, 168,  35,  89, 161,  36,  51,\n",
       "        79,  94, 246, 217,  25, 112, 206,  19, 125,  96, 199,  97, 142,\n",
       "       202, 229, 136,  53,  76,  20, 223, 140,  95, 212,  72, 197,  86,\n",
       "       139, 117,  66,  67, 171, 122, 120, 147, 100, 213,  28,  82, 145,\n",
       "       134,   9,   7, 239, 215, 196, 176,  41, 135, 182,  77,  44,  69,\n",
       "       138, 179,  21, 226, 183, 109, 159,   1, 175,  68, 225,  46,  17,\n",
       "       119,  42, 126,  43, 144, 107,  65, 163, 178, 180, 185, 132, 186,\n",
       "       174, 224,  93,  22, 200,  73, 222,  74, 110, 133,  54,  50,   8,\n",
       "       194,  87, 137, 101, 131,  23, 190,  55, 121,  10,  80,  58,  33,\n",
       "       154, 124, 220,  40,  38, 128,  70,   2, 173, 103,  98,  29,  26,\n",
       "        12, 172,   6,  62, 216, 146, 151, 247, 158, 143, 232,  31, 169,\n",
       "       170,  63,  90,  91,  32,  34, 211, 221, 245,   5, 231,  37,  88, 108])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle new_half_batch_1 to random order\n",
    "idx2 = np.arange(len(new_batch_1))\n",
    "np.random.shuffle(idx2)\n",
    "idx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffled_new_batch_1 = np.ndarray(new_batch_1.shape, dtype=float)\n",
    "shuffled_target_1_and_0 = np.ndarray(target_1_and_0.shape, dtype=float)\n",
    "for ii, index in enumerate(idx2):\n",
    "    shuffled_new_batch_1[ii, :, :] = new_batch_1[index]\n",
    "    shuffled_target_1_and_0[ii] = target_1_and_0[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gait_data/casia_b_fc8_124/shuffled_new_batch_1', 'wb') as f:\n",
    "    np.save(f, shuffled_new_batch_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_hot_shuffled_new_batch_1_target = np.ndarray([shuffled_target_1_and_0.shape[0], 2], dtype=float)\n",
    "for ii, each in enumerate(shuffled_target_1_and_0):\n",
    "    if int(each) == 1:\n",
    "        one_hot_shuffled_new_batch_1_target[ii, :] = 0.9, 0.1\n",
    "    elif int(each) == 0:\n",
    "        one_hot_shuffled_new_batch_1_target[ii, :] = 0.1, 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gait_data/casia_b_fc8_124/one_hot_shuffled_new_batch_1_target', 'wb') as f:\n",
    "    np.save(f, one_hot_shuffled_new_batch_1_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_seq_length = np.array(p_seq_length)\n",
    "shuffled_p_seq_length = np.zeros(p_seq_length.shape, dtype=int)\n",
    "for ii, index in enumerate(idx):\n",
    "    shuffled_p_seq_length[ii] = p_seq_length[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_column1 = np.reshape(np.concatenate([g_seq_length, p_seq_length], axis=0), [len(g_seq_length)*2, 1])\n",
    "seq_column2 = np.reshape(np.concatenate([g_seq_length, shuffled_p_seq_length], axis=0), [len(g_seq_length)*2, 1])\n",
    "seq_length = np.concatenate([seq_column1, seq_column2], axis=1)\n",
    "\n",
    "shuffled_seq_length = np.zeros(seq_length.shape, dtype=int)\n",
    "for ii, index in enumerate(idx2):\n",
    "    shuffled_seq_length[ii] = seq_length[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('gait_data/casia_b_fc8_124/shuffled_seq_length', 'wb') as f:\n",
    "    np.save(f, shuffled_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(248, 164, 1000)\n"
     ]
    }
   ],
   "source": [
    "seq_length = np.load(open(r'../gait_data/casia_b_fc8_124/degree_72_normal_walk/shuffled_seq_length', mode='rb'))\n",
    "shuffled_new_batch_1 = np.load(open(r'../gait_data/casia_b_fc8_124/degree_72_normal_walk/shuffled_new_batch_1', mode='rb'))\n",
    "one_hot_shuffled_new_batch_1_target = np.load(open(r'../gait_data/casia_b_fc8_124/degree_72_normal_walk/one_hot_shuffled_new_batch_1_target', mode='rb'))\n",
    "print(shuffled_new_batch_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Train and Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data length: 248\n",
      "every length has : (248, 164, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(\"data length: {}\".format(len(shuffled_new_batch_1)))\n",
    "print(\"every length has : {}\".format(shuffled_new_batch_1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_x: (198, 164, 1000)\n",
      "Validation_x: (50, 164, 1000)\n",
      "Train_y: (198, 2)\n",
      "Validation_y: (50, 2)\n",
      "train_seq_length: (198, 2)\n",
      "val_seq_length: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_num = int(split_frac*len(shuffled_new_batch_1))\n",
    "\n",
    "train_x_raw, val_x_raw = shuffled_new_batch_1[:split_num], shuffled_new_batch_1[split_num:]\n",
    "train_y_raw, val_y_raw = one_hot_shuffled_new_batch_1_target[:split_num], one_hot_shuffled_new_batch_1_target[split_num:]\n",
    "train_seq_length, val_seq_length = seq_length[:split_num], seq_length[split_num:]\n",
    "\n",
    "# print size\n",
    "print(\"Train_x: {}\".format(np.array(train_x_raw).shape))\n",
    "print(\"Validation_x: {}\".format(np.array(val_x_raw).shape))\n",
    "print(\"Train_y: {}\".format(np.array(train_y_raw).shape))\n",
    "print(\"Validation_y: {}\".format(np.array(val_y_raw).shape))\n",
    "print(\"train_seq_length: {}\".format(np.array(train_seq_length).shape))\n",
    "print(\"val_seq_length: {}\".format(np.array(val_seq_length).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# According_len_get_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def According_len_get_outputs(length_batch_1, length_batch_2, outputs_1, outputs_2):\n",
    "    \"\"\"\n",
    "    length_batch_1 and length_batch_2 should minus one\n",
    "    \"\"\"\n",
    "    new_outputs_1 = np.ndarray([outputs_1.shape[0], outputs_1.shape[2]], dtype=float)\n",
    "    new_outputs_2 = np.ndarray([outputs_2.shape[0], outputs_2.shape[2]], dtype=float)\n",
    "    \n",
    "    for ii, (L1, L2, O1, O2) in enumerate(zip(length_batch_1, length_batch_2, outputs_1, outputs_2)):\n",
    "        new_outputs_1[ii, :] = O1[L1, :]\n",
    "        new_outputs_2[ii, :] = O2[L2, :]\n",
    "        \n",
    "    return new_outputs_1, new_outputs_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(batch_size=20, lstm_size=1000, num_layers=1, learning_rate=0.001, \\\n",
    "              grad_clip=1, alpha=0.2):\n",
    "    \n",
    "    # reset graph\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    with tf.name_scope('inputs_1'):\n",
    "        inputs_1 = tf.placeholder(tf.float32, [None, None, 1000], name=\"inputs_1\")\n",
    "    with tf.name_scope('inputs_2'):\n",
    "        inputs_2 = tf.placeholder(tf.float32, [None, None, 1000], name=\"inputs_2\")\n",
    "    with tf.name_scope('targets'):\n",
    "        target_ = tf.placeholder(tf.float32, [None, 2], name=\"targets\")\n",
    "        \n",
    "    seq_1 = tf.placeholder(tf.float32, (None,), name=\"seq_1\")\n",
    "    seq_2 = tf.placeholder(tf.float32, (None,), name=\"seq_2\")\n",
    "    \n",
    "#     new_outputs_1 = tf.placeholder(tf.float32, [None, lstm_size], name=\"new_outputs_1\")\n",
    "#     new_outputs_2 = tf.placeholder(tf.float32, [None, lstm_size], name=\"new_outputs_2\")\n",
    "    \n",
    "#     new_final_state_1 = tf.placeholder(tf.float32, [None, lstm_size], name=\"new_final_state_1\")\n",
    "#     new_final_state_2 = tf.placeholder(tf.float32, [None, lstm_size], name=\"new_final_state_2\")\n",
    "    final_state_concat = tf.placeholder(tf.float32, [None, int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size)), 1], \\\n",
    "                                       name='final_state_concat')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    with tf.variable_scope(\"LSTM\"):\n",
    "#         with tf.name_scope(\"RNN_cells_1\"):\n",
    "            # Your basic LSTM cell\n",
    "        #         lstm = tf.contrib.rnn.LSTMBlockFusedCell(lstm_size, forget_bias=1.0, use_peephole=True)\n",
    "        #         lstm = tf.nn.relu(lstm)\n",
    "        lstm_1 = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "#         batch_norm_1 = tf.layers.batch_normalization(lstm_1, reuse=True)\n",
    "            # Add dropout to the cell\n",
    "        drop_1 = tf.contrib.rnn.DropoutWrapper(lstm_1, output_keep_prob=keep_prob)\n",
    "            # Stack up multiple LSTM layers, for deep learning\n",
    "        cell_1 = tf.contrib.rnn.MultiRNNCell([drop_1] * num_layers)\n",
    "\n",
    "#         with tf.name_scope(\"RNN_init_state_1\"):\n",
    "            # Getting an initial state of all zeros\n",
    "        initial_state_1 = cell_1.zero_state(batch_size, tf.float32)\n",
    "#         with tf.name_scope(\"RNN_forward_1\"):\n",
    "        outputs_1, final_state_1 = tf.nn.dynamic_rnn(cell_1, inputs_1, initial_state=initial_state_1, \n",
    "                                                     sequence_length=seq_1)\n",
    "\n",
    "    with tf.variable_scope(\"LSTM\", reuse=True):\n",
    "#         with tf.name_scope(\"RNN_cells_2\"):\n",
    "            # Your basic LSTM cell\n",
    "    #         lstm = tf.contrib.rnn.LSTMBlockFusedCell(lstm_size, forget_bias=1.0, use_peephole=True)\n",
    "    #         lstm = tf.nn.relu(lstm)\n",
    "        lstm_2 = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "#         batch_norm_2 = tf.layers.batch_normalization(lstm_2, reuse=True)\n",
    "            # Add dropout to the cell\n",
    "        drop_2 = tf.contrib.rnn.DropoutWrapper(lstm_2, output_keep_prob=keep_prob)\n",
    "            # Stack up multiple LSTM layers, for deep learning\n",
    "        cell_2 = tf.contrib.rnn.MultiRNNCell([drop_2] * num_layers)\n",
    "\n",
    "#         with tf.name_scope(\"RNN_init_state_2\"):\n",
    "            # Getting an initial state of all zeros\n",
    "        initial_state_2 = cell_2.zero_state(batch_size, tf.float32)\n",
    "\n",
    "#         with tf.name_scope(\"RNN_forward_2\"):\n",
    "        # shape [1, 2, batch_size, lstm_size]\n",
    "        outputs_2, final_state_2 = tf.nn.dynamic_rnn(cell_2, inputs_2, initial_state=initial_state_2, \n",
    "                                                     sequence_length=seq_2)\n",
    "\n",
    "    with tf.name_scope('predictions'):\n",
    "        # only choose final one\n",
    "#         final_output_concat = tf.concat([outputs_1[:, -1], outputs_2[:, -1]], 1)\n",
    "\n",
    "        # through the According_len_get_outputs\n",
    "    \n",
    "        # subtraction\n",
    "#         final_output_concat = abs(new_final_state_1 - new_final_state_2)\n",
    "#         final_output_concat = abs(new_outputs_1 - new_outputs_2)\n",
    "        \n",
    "#         # concatenation\n",
    "#         final_output_concat = tf.concat([new_outputs_1, new_outputs_2], 1)\n",
    "\n",
    "        # Conv pool_1\n",
    "        conv_1 = tf.layers.conv2d(final_state_concat, 16, 2, strides=2, padding='same')\n",
    "#         conv_1 = tf.maximum(alpha * conv_1, conv_1)\n",
    "        conv_1 = tf.nn.relu(conv_1)\n",
    "        pool_1 = tf.nn.max_pool(conv_1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        \n",
    "        # Conv pool_2\n",
    "        conv_2 = tf.layers.conv2d(pool_1, 32, 2, strides=2, padding='same')\n",
    "        conv_2 = tf.layers.batch_normalization(conv_2, training=is_training)\n",
    "#         conv_2 = tf.maximum(alpha * conv_2, conv_2)\n",
    "        conv_2 = tf.nn.relu(conv_2)\n",
    "        pool_2 = tf.nn.max_pool(conv_2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        \n",
    "        # Conv pool_3\n",
    "        conv_3 = tf.layers.conv2d(pool_2, 64, 2, strides=2, padding='same')\n",
    "        conv_3 = tf.layers.batch_normalization(conv_3, training=is_training)\n",
    "#         conv_3 = tf.maximum(alpha * conv_3, conv_3)\n",
    "        conv_3 = tf.nn.relu(conv_3)\n",
    "        pool_3 = tf.nn.max_pool(conv_3, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "        \n",
    "        # Flatten\n",
    "        pool_3 = tf.reshape(pool_3, [-1, pool_3.get_shape().as_list()[1]*pool_3.get_shape().as_list()[2]*\\\n",
    "                                     pool_3.get_shape().as_list()[3]])\n",
    "        \n",
    "        # Fully_connected\n",
    "        f_1 = tf.contrib.layers.fully_connected(pool_3, 100)\n",
    "        logits = tf.contrib.layers.fully_connected(f_1, 2)\n",
    "        predictions = tf.nn.softmax(logits, name='predictions')\n",
    "        tf.summary.histogram('predictions', predictions)\n",
    "        \n",
    "#     with tf.name_scope('accuracy'):\n",
    "#         correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(target_, 1))\n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target_, logits=logits))\n",
    "        tf.summary.scalar('cost', cost)\n",
    "        \n",
    "    with tf.name_scope('train'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip, name='clip')\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "        optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "#         optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope('saver'):\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    export_nodes = ['inputs_1', 'inputs_2', 'target_', 'initial_state_1', 'initial_state_2', 'outputs_1', 'outputs_2',\n",
    "                    'final_state_1', 'final_state_2', 'keep_prob', 'cost', 'logits', 'predictions', 'optimizer', \n",
    "                    'merged', 'saver', 'learning_rate', 'cell_1', 'cell_2', 'seq_1', 'seq_2', 'final_state_concat',\n",
    "                   'is_training']\n",
    "    \n",
    "    Graph = collections.namedtuple('Graph', export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(data, target, seq_length, batch_size):\n",
    "    n_batches = len(data)//batch_size\n",
    "    data = data[:batch_size * n_batches]\n",
    "    target = target[:batch_size * n_batches]\n",
    "    for ii in range(0, batch_size*n_batches, batch_size):\n",
    "        data_batch = data[ii:ii + batch_size]\n",
    "        target_batch = target[ii:ii + batch_size]\n",
    "        seq_length_batch_1 = seq_length[ii:ii + batch_size, 0]\n",
    "        seq_length_batch_2 = seq_length[ii:ii + batch_size, 1]\n",
    "        \n",
    "        yield data_batch, target_batch, seq_length_batch_1, seq_length_batch_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x_raw\n",
    "val_x = val_x_raw\n",
    "train_y = train_y_raw\n",
    "val_y = val_y_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 164, 1000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_acc(pred, target):\n",
    "    correct_pred = np.equal(np.argmax(pred, axis=1), np.argmax(target, axis=1))\n",
    "    acc = np.mean(correct_pred.astype(float)) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, file_writer, save_string):\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        iteration = 1\n",
    "        mean_val_Acc = 0\n",
    "        mean_val_loss = 0\n",
    "        count_Acc_not_increase_epochs = 0\n",
    "        count_loss_not_decrease_epochs = 0\n",
    "        Last_val_Acc = 0\n",
    "        Last_val_loss = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for e in range(epochs):\n",
    "            for train_batch, target_batch, seq_length_batch_1, seq_length_batch_2 \\\n",
    "                             in get_batches(train_x, train_y, train_seq_length, batch_size):\n",
    "                    \n",
    "                start = time.time()\n",
    "                \n",
    "#                 print(train_batch[:, :15, :].shape)\n",
    "#                 print(target_batch)\n",
    "    #             print('target: {}'.format(target_batch.shape))\n",
    "                state_1 = sess.run(model.cell_1.zero_state(batch_size, tf.float32))\n",
    "                state_2 = sess.run(model.cell_2.zero_state(batch_size, tf.float32))\n",
    "\n",
    "                feed_1 = {\n",
    "                        model.inputs_1: train_batch[:, :82, :],\n",
    "                        model.inputs_2: train_batch[:, 82:, :],  \n",
    "                        model.seq_1: seq_length_batch_1,\n",
    "                        model.seq_2: seq_length_batch_2,\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state_1: state_1,\n",
    "                        model.initial_state_2: state_2,\n",
    "                        model.is_training: True\n",
    "                        }\n",
    "            \n",
    "#                 print(model.outputs_1.shape, model.outputs_1[:, -1], model.final_output_concat, \n",
    "#                       model.logits, model.predictions, model.cost)\n",
    "                f_1, f_2 = sess.run([model.final_state_1, model.final_state_2], feed_dict=feed_1)\n",
    "                \n",
    "#                 state_1 = f_1\n",
    "#                 state_2 = f_2\n",
    "\n",
    "                show_f_state_1 = np.reshape(f_1[0][1], [batch_size, \\\n",
    "                                                        int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size)), 1])\n",
    "                show_f_state_2 = np.reshape(f_2[0][1], [batch_size, \\\n",
    "                                                        int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size)), 1])\n",
    "                show_final_state = abs(show_f_state_1 - show_f_state_2)\n",
    "#                 print('show_output:', target_batch[1])\n",
    "#                 plt.imshow(np.reshape(show_final_state[1], [int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size))]))\n",
    "#                 plt.show()\n",
    "#                 real_outputs_1, real_outputs_2 = According_len_get_outputs(\n",
    "#                                                seq_length_batch_1 - 1, seq_length_batch_2 - 1, o_1, o_2)\n",
    "            \n",
    "#                 show_output_1 = np.reshape(real_outputs_1[0], [50, 60])\n",
    "#                 show_output_2 = np.reshape(real_outputs_2[0], [50, 60])\n",
    "#                 print('show_output:', target_batch[0])\n",
    "#                 plt.imshow(abs(show_output_1 - show_output_2))\n",
    "#                 plt.show()\n",
    "    \n",
    "#                 print(o_1[:, 28], o_2[:, 28])\n",
    "#                 print(real_outputs_1, real_outputs_2)\n",
    "                feed_2 = {\n",
    "                        model.target_: target_batch,\n",
    "                        model.keep_prob: 0.5,\n",
    "#                         model.initial_state_1: state_1,\n",
    "#                         model.initial_state_2: state_2,\n",
    "                        model.learning_rate: lr,\n",
    "                        model.final_state_concat: show_final_state,\n",
    "                        model.is_training: True\n",
    "#                         model.new_outputs_1: real_outputs_1,\n",
    "#                         model.new_outputs_2: real_outputs_2\n",
    "                        }\n",
    "                \n",
    "                loss, _, logi, pred, summary = sess.run(\n",
    "                                [model.cost, model.optimizer, model.logits, model.predictions, model.merged]\n",
    "                                 , feed_dict=feed_2)\n",
    "            \n",
    "                Acc = calculate_acc(pred, target_batch)\n",
    "                \n",
    "#                 print(pred)\n",
    "#                 print(logi)\n",
    "#                 print('pred: {}'.format(type(pred)))\n",
    "    #             print('pred: {}'.format(np.count_nonzero(pred)))\n",
    "    #             print('pred: {}'.format(np.sum(pred>0.1)))\n",
    "    #             print('pred: {}'.format(pred.size))\n",
    "    #             print('output: {}'.format(out[1, 39, :]))\n",
    "#                 print(pred.shape)\n",
    "        \n",
    "                \n",
    "                if iteration%5==0:\n",
    "                    end = time.time()\n",
    "#                     acc = calculate_accuracy(sess, pred, target_batch, sq_length)\n",
    "                    print(\n",
    "                          \"Epoch: {}/{},\".format(e+1, epochs),' ',\n",
    "                          \"Iteration: {},\".format(iteration),' ',\n",
    "                          \"Train loss: {:.3f},\".format(loss),' ',\n",
    "                          \"{:.1f}s /batch.\".format((end-start)/5),' '\n",
    "                          \"ACCuracy: %{}\".format(Acc)\n",
    "                         )\n",
    "                    \n",
    "                    file_writer.add_summary(summary, iteration)\n",
    "            \n",
    "                ##############################################################\n",
    "                ######################## VALIDATION ##########################\n",
    "                ##############################################################\n",
    "                \n",
    "                if iteration%15==0:\n",
    "                    training = True\n",
    "                    validation_loss = []\n",
    "                    validation_Acc = []\n",
    "                    \n",
    "                    if batch_size >= len(val_x):\n",
    "                        val_batch_size = 10\n",
    "                    else: \n",
    "                        val_batch_size = batch_size\n",
    "                                            \n",
    "                    for val_batch, val_target_batch, val_seq_length_batch_1, val_seq_length_batch_2 \\\n",
    "                             in get_batches(val_x, val_y, val_seq_length, val_batch_size):\n",
    "                            \n",
    "                        val_state_1 = sess.run(model.cell_1.zero_state(val_batch_size, tf.float32))\n",
    "                        val_state_2 = sess.run(model.cell_2.zero_state(val_batch_size, tf.float32))\n",
    "#                         print(val_state_1[0][0].shape)\n",
    "\n",
    "                        feed_3 = {\n",
    "                                model.inputs_1: val_batch[:, :82, :],\n",
    "                                model.inputs_2: val_batch[:, 82:, :],  \n",
    "                                model.seq_1: val_seq_length_batch_1,\n",
    "                                model.seq_2: val_seq_length_batch_2,\n",
    "                                model.keep_prob: 1,\n",
    "                                model.initial_state_1: val_state_1,\n",
    "                                model.initial_state_2: val_state_2,\n",
    "                                model.is_training: True\n",
    "                                }\n",
    "                        \n",
    "                        val_f_1, val_f_2 = sess.run([model.final_state_1, model.final_state_2], feed_dict=feed_3)\n",
    "                        \n",
    "                        val_show_f_state_1 = np.reshape(val_f_1[0][1], [val_batch_size, \\\n",
    "                                                        int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size)), 1])\n",
    "                        val_show_f_state_2 = np.reshape(val_f_2[0][1], [val_batch_size, \\\n",
    "                                                        int(np.sqrt(lstm_size)), int(np.sqrt(lstm_size)), 1])\n",
    "                        val_show_final_state = abs(val_show_f_state_1 - val_show_f_state_2)\n",
    "                        \n",
    "                        feed_4 = {\n",
    "                                model.target_: val_target_batch,\n",
    "                                model.keep_prob: 1,\n",
    "#                         model.initial_state_1: state_1,\n",
    "#                         model.initial_state_2: state_2,\n",
    "                                model.learning_rate: lr,\n",
    "                                model.final_state_concat: val_show_final_state,\n",
    "                                model.is_training: True\n",
    "#                         model.new_outputs_1: real_outputs_1,\n",
    "#                         model.new_outputs_2: real_outputs_2\n",
    "                                 }\n",
    "                        \n",
    "                        val_loss, val_pred = sess.run([model.cost, model.predictions], feed_dict=feed_4)\n",
    "                        \n",
    "                        val_acc = calculate_acc(val_pred, val_target_batch)\n",
    "\n",
    "                        validation_loss.append(val_loss)\n",
    "                        \n",
    "                        validation_Acc.append(val_acc)\n",
    "                        \n",
    "                    Last_val_Acc = mean_val_Acc\n",
    "                    Last_val_loss = mean_val_loss\n",
    "                        \n",
    "                    mean_val_loss = sum(validation_loss)/len(validation_loss)\n",
    "                    mean_val_Acc = sum(validation_Acc)/len(validation_Acc)\n",
    "                    print()\n",
    "                    print(\"Validation loss: {:.3f},\".format(mean_val_loss), ' ',\n",
    "                          \"Validation Accuracy: %{:.3f}\".format(mean_val_Acc))\n",
    "                    print()\n",
    "                    \n",
    "                iteration += 1\n",
    "                \n",
    "#             #  Early stopping  \n",
    "#             if mean_val_Acc - Last_val_Acc <= -0.5:\n",
    "#                 count_Acc_not_increase_epochs += 1\n",
    "#             if Last_val_loss - mean_val_loss <= -0.03:\n",
    "#                 count_loss_not_decrease_epochs += 1\n",
    "                \n",
    "#             if mean_val_Acc - Last_val_Acc <= -2:\n",
    "#                 break\n",
    "#             if count_Acc_not_increase_epochs >= 50:\n",
    "#                 break\n",
    "#             if count_loss_not_decrease_epochs >= 50:\n",
    "#                 break\n",
    "                \n",
    "        model.saver.save(sess, r\"{}\".format(save_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoints_identification: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1500,   Iteration: 5,   Train loss: 0.709,   0.3s /batch.  ACCuracy: %40.0\n",
      "Epoch: 2/1500,   Iteration: 10,   Train loss: 0.650,   0.4s /batch.  ACCuracy: %56.666666666666664\n",
      "Epoch: 3/1500,   Iteration: 15,   Train loss: 0.574,   0.4s /batch.  ACCuracy: %80.0\n",
      "\n",
      "Validation loss: 0.617,   Validation Accuracy: %73.333\n",
      "\n",
      "Epoch: 4/1500,   Iteration: 20,   Train loss: 0.542,   0.3s /batch.  ACCuracy: %80.0\n",
      "Epoch: 5/1500,   Iteration: 25,   Train loss: 0.547,   0.3s /batch.  ACCuracy: %90.0\n",
      "Epoch: 5/1500,   Iteration: 30,   Train loss: 0.497,   0.3s /batch.  ACCuracy: %83.33333333333334\n",
      "\n",
      "Validation loss: 0.591,   Validation Accuracy: %73.333\n",
      "\n",
      "Epoch: 6/1500,   Iteration: 35,   Train loss: 0.515,   0.3s /batch.  ACCuracy: %80.0\n",
      "Epoch: 7/1500,   Iteration: 40,   Train loss: 0.484,   0.3s /batch.  ACCuracy: %86.66666666666667\n",
      "Epoch: 8/1500,   Iteration: 45,   Train loss: 0.426,   0.3s /batch.  ACCuracy: %93.33333333333333\n",
      "\n",
      "Validation loss: 0.495,   Validation Accuracy: %80.000\n",
      "\n",
      "Epoch: 9/1500,   Iteration: 50,   Train loss: 0.450,   0.3s /batch.  ACCuracy: %90.0\n",
      "Epoch: 10/1500,   Iteration: 55,   Train loss: 0.477,   0.3s /batch.  ACCuracy: %86.66666666666667\n",
      "Epoch: 10/1500,   Iteration: 60,   Train loss: 0.384,   0.3s /batch.  ACCuracy: %100.0\n",
      "\n",
      "Validation loss: 0.508,   Validation Accuracy: %83.333\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ff708f959c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m                                   \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-2c60047a90e8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, file_writer, save_string)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#                 print(model.outputs_1.shape, model.outputs_1[:, -1], model.final_output_concat,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#                       model.logits, model.predictions, model.cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mf_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#                 state_1 = f_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training = True\n",
    "degree = 72\n",
    "epochs = 1500\n",
    "batch_size = 30\n",
    "for lstm_size in [625]: # 625 91%\n",
    "    for num_layers in [1]:\n",
    "        for lr in [0.001]:\n",
    "                log_string = 'logs/4/dg={},lr={},nl={},ls={},bs={}'.format(degree, lr, num_layers, lstm_size, batch_size)\n",
    "                save_string = 'checkpoints\\dg={}_lr={}_nl={}_ls={}_bs={}.ckpt'.format(degree, lr, num_layers, lstm_size, batch_size)\n",
    "                \n",
    "                writer = tf.summary.FileWriter(log_string)\n",
    "                                \n",
    "                model = build_rnn(batch_size=batch_size, lstm_size=lstm_size, \\\n",
    "                                  num_layers=num_layers, learning_rate=lr, grad_clip=1)\n",
    "\n",
    "                train(model, epochs, writer, save_string)\n",
    "                \n",
    "                print(' ')\n",
    "                print(' ')\n",
    "                print(\"leraning_rate={},num_layers={},lstm_size={},batch_size={} finished, saved\".format(lr, num_layers, lstm_size, batch_size))\n",
    "                print(' ')\n",
    "                print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
