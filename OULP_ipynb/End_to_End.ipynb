{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restore individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from scipy.misc import imsave\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pickle\n",
    "# from tensorflow_vgg import vgg16\n",
    "import collections\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auto_Encoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=1_lr=0.0013_nl=2_bs=80.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au1w1 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_1:0').eval()\n",
    "    Au1b1 = loaded_graph.get_tensor_by_name('conv_layers/bias_1:0').eval()\n",
    "    Au1w2 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_2:0').eval()\n",
    "    Au1b2 = loaded_graph.get_tensor_by_name('conv_layers/bias_2:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=2_lr=0.001_nl=2_bs=80.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au2deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au2deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=3_lr=0.001_nl=2_bs=80.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au3deW1 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_1:0').eval()\n",
    "    Au3deB1 = loaded_graph.get_tensor_by_name('deconv_layers/debias_1:0').eval()\n",
    "    Au3deW2 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_2:0').eval()\n",
    "    Au3deB2 = loaded_graph.get_tensor_by_name('deconv_layers/debias_2:0').eval()\n",
    "    Au3outW = loaded_graph.get_tensor_by_name('output_layer/output_weights:0').eval()\n",
    "    Au3outb = loaded_graph.get_tensor_by_name('output_layer/outbias_1:0').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gallery_full = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_full_Seq00', mode='rb'))\n",
    "probe_full = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_full_Seq01', mode='rb'))\n",
    "gallery_full = gallery_full/float(255)\n",
    "probe_full = probe_full/float(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (5600, 64, 64, 1) train_y (5600, 64, 64, 1)\n",
      "val_x (346, 64, 64, 1) val_y (346, 64, 64, 1)\n",
      "test_x (346, 64, 64, 1) test_y (346, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train X\n",
    "train_x = np.concatenate([gallery_full[:2800, 0], probe_full[:2800, 0]], axis=0)\n",
    "train_x = np.reshape(train_x, [train_x.shape[0], train_x.shape[1], train_x.shape[2], 1])\n",
    "# Train Y\n",
    "train_y = np.concatenate([gallery_full[:2800, 3], probe_full[:2800, 3]], axis=0)\n",
    "train_y = np.reshape(train_x, [train_x.shape[0], train_x.shape[1], train_x.shape[2], 1])\n",
    "print('train_x', train_x.shape, 'train_y', train_y.shape)\n",
    "\n",
    "# Val X\n",
    "val_x = gallery_full[2800:, 0]\n",
    "val_x = np.reshape(val_x, [val_x.shape[0], val_x.shape[1], val_x.shape[2], 1])\n",
    "# Val Y\n",
    "val_y = gallery_full[2800:, 3]\n",
    "val_y = np.reshape(val_y, [val_y.shape[0], val_y.shape[1], val_y.shape[2], 1])\n",
    "print('val_x', val_x.shape, 'val_y', val_y.shape)\n",
    "\n",
    "# Test X\n",
    "test_x = probe_full[2800:, 0]\n",
    "test_x = np.reshape(test_x, [test_x.shape[0], test_x.shape[1], test_x.shape[2], 1])\n",
    "# Test Y\n",
    "test_y = probe_full[2800:, 3]\n",
    "test_y = np.reshape(test_x, [test_x.shape[0], test_x.shape[1], test_x.shape[2], 1])\n",
    "print('test_x', test_x.shape, 'test_y', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(inputs_, weight, weight_name, bias, bias_name, conv_name, pool_name, bn_name, relu_name, add_bacth_norm=True, is_training=True,\n",
    "               out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), pool_size=(2,2), pool_strides=(2,2), keep_prob=0.5):\n",
    "    ### Encoder\n",
    "    # e.g. out_dim: 8; kern_size: (3,3); pool_size: (2,2); strides: (2,2);\n",
    "    \n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    # filter:[filter_height, filter_width, in_channels, out_channels]\n",
    "    # input:[batch, in_height, in_width, in_channels]\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(inputs_, Weights, stride, padding='SAME', name=conv_name), Bias)\n",
    "    \n",
    "    # Now 28x28x\n",
    "    if add_bacth_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "        \n",
    "    # add activation function\n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    # add Max pooling\n",
    "    conv2d_maxpool = tf.nn.max_pool(conv2d, [1,pool_size[0],pool_size[1],1],\\\n",
    "                                    [1,pool_strides[0],pool_strides[1],1], padding='SAME', name=pool_name)\n",
    "                    \n",
    "    conv2d_maxpool = tf.layers.dropout(conv2d_maxpool, rate=keep_prob)\n",
    "\n",
    "    return conv2d_maxpool\n",
    "\n",
    "def deconv_layer(encoded, up_name, weight, weight_name, bias, bias_name, de_conv_name, bn_name, relu_name, add_batch_norm=True, is_training=True,\n",
    "                 up_size=[(2,2),(4,4),(5,5),(6,6)], out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), keep_prob=0.5):\n",
    "    ### Decoder\n",
    "    upsample = tf.image.resize_nearest_neighbor(encoded, up_size, name=up_name) # up_size: e.g.(7,7)\n",
    "    \n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(upsample, Weights, stride, padding='SAME', name=de_conv_name),\\\n",
    "                   Bias)\n",
    "    \n",
    "    if add_batch_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "    \n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    conv2d = tf.layers.dropout(conv2d, rate=keep_prob)\n",
    "    \n",
    "    return conv2d\n",
    "\n",
    "def output_layer(conv2d, weight, weight_name, bias, bias_name, logits_name, bn_name, decoded_name, add_batch_norm=True, is_training=True, \n",
    "                 out_dim=1, conv_kernsize=(3,3), conv_strides=(1,1)):\n",
    "    # Weights\n",
    "    Weights = tf.Variable(weight, name=weight_name)\n",
    "    \n",
    "    # Bias\n",
    "    Bias = tf.Variable(bias, name=bias_name)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    logits = tf.nn.bias_add(tf.nn.conv2d(conv2d, Weights, stride, padding='SAME', name=logits_name),\\\n",
    "                   Bias)\n",
    "    \n",
    "    if add_batch_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "    #Now 28x28x1\n",
    "    decoded = tf.nn.sigmoid(logits, name=decoded_name)\n",
    "    \n",
    "    return logits, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_kern_size = (4,4)\n",
    "conv_strides = (1,1)\n",
    "pool_size = (2,2)\n",
    "pool_strides = (2,2)\n",
    "up_size = [(64,64)]\n",
    "de_kern_size = (4,4)\n",
    "de_conv_strides = (1,1)\n",
    "out_kern_size = (4,4)\n",
    "out_strides = (1,1)\n",
    "\n",
    "\n",
    "g = tf.Graph()\n",
    "tf.reset_default_graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    Au1_Weights_1 = tf.convert_to_tensor(Au1w1, dtype=tf.float32)\n",
    "    Au1_Bias_1 = tf.convert_to_tensor(Au1b1, dtype=tf.float32)\n",
    "    Au1_Weights_2 = tf.convert_to_tensor(Au1w2, dtype=tf.float32)\n",
    "    Au1_Bias_2 = tf.convert_to_tensor(Au1b2, dtype=tf.float32)\n",
    "    Au2_deWeights_1 = tf.convert_to_tensor(Au2deW1, dtype=tf.float32)\n",
    "    Au2_deBias_1 = tf.convert_to_tensor(Au2deB1, dtype=tf.float32)\n",
    "    Au3_deWeights_1 = tf.convert_to_tensor(Au3deW1, dtype=tf.float32)\n",
    "    Au3_deBias_1 = tf.convert_to_tensor(Au3b2, dtype=tf.float32)\n",
    "    Au3_DeWeights_2 = tf.convert_to_tensor(Au3deW2, dtype=tf.float32)\n",
    "    Au3_DeBiass_2 = tf.convert_to_tensor(Au3deB2, dtype=tf.float32)\n",
    "    Au3_OutWeights = tf.convert_to_tensor(Au3outW, dtype=tf.float32)\n",
    "    Au3_OutBias = tf.convert_to_tensor(Au3outb, dtype=tf.float32)\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_p = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    training = tf.placeholder(tf.bool, name='training')\n",
    "    inputs = tf.placeholder(tf.float32, [None, 64, 64, 1], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, 64, 64, 1], name='targets')\n",
    "\n",
    "    # 64x64 -- 32x32\n",
    "    with tf.name_scope('conv_layer'):\n",
    "        # outdim 1 -- 16\n",
    "        maxpool_1 = conv_layer(inputs, Au1_Weights_1, 'weight_1', Au1_Bias_1, 'bias_1', 'conv_1', 'pool_1', 'bn_1', 'relu_1', add_bacth_norm=False, \n",
    "                               is_training=training, out_dim=16, conv_kernsize=conv_kern_size, conv_strides=conv_strides, \n",
    "                               pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
    "    \n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        # Now 32x32x16 Hidden Layer 1 Au_1\n",
    "        # outdim 16 -- 8\n",
    "        stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "\n",
    "        Au1Weights2 = tf.Variable(Au1_Weights_2, name='h_weight_1')\n",
    "        Au1Bias2 = tf.Variable(Au1_Bias_2, name='h_bias_1')\n",
    "        conv_hidden_1 = tf.nn.bias_add(tf.nn.conv2d(maxpool_1, Au1Weights2, stride, padding='SAME', name='hidden_1'), Au1Bias2)\n",
    "\n",
    "        # Now 32x32x8 Hidden Layer 2 Au_2\n",
    "        # outdim 8 -- 8\n",
    "        Au2deWeights1 = tf.Variable(Au2_deWeights_1, name='h_weight_2')\n",
    "        Au2deBias1 = tf.Variable(Au2_deBias_1, name='h_bias_2')\n",
    "        conv_hidden_2 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_1, Au2deWeights1, stride, padding='SAME', name='hidden_2'), Au2deBias1)\n",
    "\n",
    "        # Now 32x32x8 Hidden Layer 3 Au_3\n",
    "        # outdim 8 -- 8\n",
    "        Au3deWeights1 = tf.Variable(Au3_deWeights_1, name='h_weight_3')\n",
    "        Au3deBias1 = tf.Variable(Au3_deBias_1, name='h_bias_3')\n",
    "        conv_hidden_3 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_2, Au3deWeights1, stride, padding='SAME', name='hidden_3'), Au3deBias1)\n",
    "    \n",
    "    with tf.name_scope('deconv_layers'):\n",
    "        # Now 32x32x8 Deconv Layer\n",
    "        # outdim 8 -- 16\n",
    "        deconv_1 = deconv_layer(conv_hidden_3, 'up_1', Au3_DeWeights_2, 'deWeight_1', Au3_DeBiass_2, 'deBias_1', 'deconv_1', 'debn_1', 'derelu_1', add_batch_norm=True, is_training=training,\n",
    "                     up_size=up_size[0], out_dim=16, conv_kernsize=de_kern_size, conv_strides=de_conv_strides, keep_prob=keep_p)\n",
    "   \n",
    "    with tf.name_scope('output_layer'):\n",
    "        # Now 64x64x16 Output Layer\n",
    "        # outdim 16 -- 1\n",
    "        logits, decoded = output_layer(deconv_1, Au3_OutWeights, 'outWeight', Au3_OutBias, 'outBias', 'logits', 'outbn', 'output', add_batch_norm=True, is_training=training, \n",
    "                                       out_dim=1, conv_kernsize=out_kern_size, conv_strides=out_strides)\n",
    "\n",
    "    # Now 64x64x1 Output\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    with tf.name_scope('saver'):\n",
    "        saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(data, target, batch_size):\n",
    "    n_batches = len(data)//batch_size\n",
    "    data = data[:batch_size * n_batches]\n",
    "    target = target[:batch_size * n_batches]\n",
    "    for ii in range(0, batch_size*n_batches, batch_size):\n",
    "        data_batch = data[ii:ii + batch_size]\n",
    "        target_batch = target[ii:ii + batch_size]\n",
    "        \n",
    "        yield data_batch, target_batch\n",
    "        \n",
    "        \n",
    "def cal_accuracy(decoded, target):\n",
    "    error = abs(decoded - target)\n",
    "    Acc = (np.sum(error <= 0.08)/(decoded.shape[0]*decoded.shape[1]*decoded.shape[2]*decoded.shape[3]))*100\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1800,  Iteration: 25,  Train loss: 0.368,       0.2s /batch.  Train Accuracy: %70.576\n",
      "Epoch: 1/1800,  Iteration: 50,  Train loss: 0.265,       0.2s /batch.  Train Accuracy: %76.490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-a6d3e40d6a93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                         }\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1800\n",
    "batch_size = 80\n",
    "\n",
    "for lr in [0.001]:\n",
    "    save_string = './checkpoints_view_invariant/E2E_lr={}_bs={}.ckpt'.format(lr, batch_size)\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True), graph=g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration = 1\n",
    "        mean_val_loss = 0\n",
    "        mean_val_acc = 0\n",
    "        count_loss_not_decrease_epochs = 0\n",
    "        count_acc_not_decrease_epochs = 0\n",
    "        Last_val_loss = 0\n",
    "        Last_val_acc = 0\n",
    "\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for train_batch, target_batch in get_batches(train_x, train_y, batch_size):\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                feed_1 = {\n",
    "                            inputs: train_batch, \n",
    "                            targets: target_batch,\n",
    "                            keep_p: 0.6,\n",
    "                            learning_rate: lr,\n",
    "                            training:True\n",
    "                        }\n",
    "\n",
    "                train_loss, _, decoded_img = sess.run([cost, opt, decoded], feed_dict=feed_1)\n",
    "\n",
    "                train_acc = cal_accuracy(decoded_img, target_batch)\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    end = time.time()\n",
    "                    print(\"Epoch: {}/{},\".format(e+1, epochs),' '\n",
    "                              \"Iteration: {},\".format(iteration),' '\n",
    "                              \"Train loss: {:.3f},\".format(train_loss),'      '\n",
    "                              \"{:.1f}s /batch.\".format((end-start)/5),' '\n",
    "                              \"Train Accuracy: %{:.3f}\".format(train_acc))\n",
    "\n",
    "                    ##############################################################\n",
    "                    ######################## VALIDATION ##########################\n",
    "                    ##############################################################\n",
    "\n",
    "                if iteration%75==0:\n",
    "                    validation_loss = []\n",
    "                    validation_acc = []\n",
    "\n",
    "                    if batch_size >= len(val_x):\n",
    "                        val_batch_size = len(val_x)\n",
    "                    else: \n",
    "                        val_batch_size = batch_size\n",
    "\n",
    "\n",
    "                    for ii, (val_batch, val_target_batch) in enumerate(get_batches(val_x, val_y, val_batch_size)):\n",
    "                        feed_2 = {\n",
    "                                    inputs: val_batch,\n",
    "                                    targets: val_target_batch,\n",
    "                                    keep_p: 1,\n",
    "                                    training:True\n",
    "                                }\n",
    "\n",
    "                        val_loss, val_decoded_img = sess.run([cost, decoded], feed_dict=feed_2)\n",
    "\n",
    "                        val_acc = cal_accuracy(val_decoded_img, val_target_batch)\n",
    "\n",
    "                        validation_loss.append(val_loss)\n",
    "                        validation_acc.append(val_acc)\n",
    "\n",
    "                    Last_val_loss = mean_val_loss\n",
    "                    Last_val_acc = mean_val_acc\n",
    "                    mean_val_loss = np.mean(np.array(validation_loss))\n",
    "                    mean_val_acc = np.mean(np.array(validation_acc))\n",
    "\n",
    "                    print()\n",
    "                    print(\"Validation loss: {:.3f},\".format(mean_val_loss),' '\n",
    "                              \"Validation accuracy: {:.3f},\".format(mean_val_acc))\n",
    "                    print()\n",
    "                ####### plot #######\n",
    "                if iteration%500==0:\n",
    "\n",
    "                    feed_3 = {\n",
    "                                inputs: val_x[:10],\n",
    "                                keep_p: 1,\n",
    "                                training:True\n",
    "                                }\n",
    "\n",
    "                    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "                    in_imgs = val_y[:10]\n",
    "                    reconstructed = sess.run(decoded, feed_dict=feed_3)\n",
    "\n",
    "                    # plot out\n",
    "                    for images, row in zip([in_imgs, reconstructed], axes):\n",
    "                        for img, ax in zip(images, row):\n",
    "                            ax.imshow(img.reshape((64, 64)), cmap='Greys_r')\n",
    "                            ax.get_xaxis().set_visible(False)\n",
    "                            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "                    fig.tight_layout(pad=0.1)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "    #             # Early stopping  \n",
    "    #             if mean_val_Acc - Last_val_Acc <= -0.3:\n",
    "    #                 count_Acc_not_increase_epochs += 1\n",
    "    #             if Last_val_loss - mean_val_loss <= -0.01:\n",
    "    #                 count_loss_not_decrease_epochs += 1\n",
    "\n",
    "    #             if mean_val_Acc - Last_val_Acc <= -2:\n",
    "    #                 break\n",
    "    #             if count_Acc_not_increase_epochs >= 10:\n",
    "    #                 break\n",
    "    #             if count_loss_not_decrease_epochs >= 10:\n",
    "    #                 break\n",
    "\n",
    "        model.saver.save(sess, r\"{}\".format(save_string))\n",
    "        \n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"leraning_rate={},num_layers={},batch_size={} finished, saved\".format(lr, num_layers, batch_size))\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
