{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Restore individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from scipy.misc import imsave\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import pickle\n",
    "# from tensorflow_vgg import vgg16\n",
    "import collections\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auto_Encoder_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=1_lr=0.0013_nl=2_bs=80.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au_1_weight_1 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_1:0')\n",
    "    Au_1_bias_1 = loaded_graph.get_tensor_by_name('conv_layers/bias_1:0')\n",
    "    Au_1_weight_2 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_2:0')\n",
    "    Au_1_bias_2 = loaded_graph.get_tensor_by_name('conv_layers/bias_2:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=2_lr=0.001_nl=2_bs=80.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au_2_weight_2 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_2:0')\n",
    "    Au_2_bias_2 = loaded_graph.get_tensor_by_name('conv_layers/bias_2:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"The name 'conv_layers/bias_2:0' refers to a Tensor which does not exist. The operation, 'conv_layers/bias_2', does not exist in the graph.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-307e2af66a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Get Tensors from loaded model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mAu_3_weight_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_layers/conv_weights_2:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mAu_3_bias_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_layers/bias_2:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mAu_3_deconvweight_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deconv_layers/deconv_weights_1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mAu_3_debias_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'deconv_layers/debias_1:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_tensor_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2620\u001b[0m       raise TypeError(\"Tensor names are strings (or similar), not %s.\"\n\u001b[1;32m   2621\u001b[0m                       % type(name).__name__)\n\u001b[0;32m-> 2622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2473\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/congcong/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2513\u001b[0m           raise KeyError(\"The name %s refers to a Tensor which does not \"\n\u001b[1;32m   2514\u001b[0m                          \u001b[0;34m\"exist. The operation, %s, does not exist in the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2515\u001b[0;31m                          \"graph.\" % (repr(name), repr(op_name)))\n\u001b[0m\u001b[1;32m   2516\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2517\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"The name 'conv_layers/bias_2:0' refers to a Tensor which does not exist. The operation, 'conv_layers/bias_2', does not exist in the graph.\""
     ]
    }
   ],
   "source": [
    "loaded_graph = tf.Graph()\n",
    "save_model_path = './checkpoints_view_invariant/Au=3_lr=0.001_nl=2_bs=80_nl=2.ckpt'\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load model\n",
    "    loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "    loader.restore(sess, save_model_path)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    Au_3_weight_2 = loaded_graph.get_tensor_by_name('conv_layers/conv_weights_2:0')\n",
    "    Au_3_bias_2 = loaded_graph.get_tensor_by_name('conv_layers/bias_2:0')\n",
    "    Au_3_deconvweight_2 = loaded_graph.get_tensor_by_name('deconv_layers/deconv_weights_2:0')\n",
    "    Au_3_debias_2 = loaded_graph.get_tensor_by_name('deconv_layers/debias_2:0')\n",
    "    Au_3_outweight = loaded_graph.get_tensor_by_name('output_layer/output_weights:0')\n",
    "    Au_3_outbias = loaded_graph.get_tensor_by_name('output_layer/outbias:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gallery_full = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_full_Seq00', mode='rb'))\n",
    "probe_full = np.load(open(r'../gait_data/OULP_GEI/GEI64x64_full_Seq01', mode='rb'))\n",
    "gallery_full = gallery_full/float(255)\n",
    "probe_full = probe_full/float(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (5600, 64, 64, 1) train_y (5600, 64, 64, 1)\n",
      "val_x (346, 64, 64, 1) val_y (346, 64, 64, 1)\n",
      "test_x (346, 64, 64, 1) test_y (346, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# Train X\n",
    "train_x = np.concatenate([gallery_full[:2800, 0], probe_full[:2800, 0]], axis=0)\n",
    "train_x = np.reshape(train_x, [train_x.shape[0], train_x.shape[1], train_x.shape[2], 1])\n",
    "# Train Y\n",
    "train_y = np.concatenate([gallery_full[:2800, 3], probe_full[:2800, 3]], axis=0)\n",
    "train_y = np.reshape(train_x, [train_x.shape[0], train_x.shape[1], train_x.shape[2], 1])\n",
    "print('train_x', train_x.shape, 'train_y', train_y.shape)\n",
    "\n",
    "# Val X\n",
    "val_x = gallery_full[2800:, 0]\n",
    "val_x = np.reshape(val_x, [val_x.shape[0], val_x.shape[1], val_x.shape[2], 1])\n",
    "# Val Y\n",
    "val_y = gallery_full[2800:, 3]\n",
    "val_y = np.reshape(val_y, [val_y.shape[0], val_y.shape[1], val_y.shape[2], 1])\n",
    "print('val_x', val_x.shape, 'val_y', val_y.shape)\n",
    "\n",
    "# Test X\n",
    "test_x = probe_full[2800:, 0]\n",
    "test_x = np.reshape(test_x, [test_x.shape[0], test_x.shape[1], test_x.shape[2], 1])\n",
    "# Test Y\n",
    "test_y = probe_full[2800:, 3]\n",
    "test_y = np.reshape(test_x, [test_x.shape[0], test_x.shape[1], test_x.shape[2], 1])\n",
    "print('test_x', test_x.shape, 'test_y', test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build End to End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.ops.variables.Variable at 0x119f6e390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([2,2,2], stddev=0.02))\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(inputs_, weight, bias, conv_name, pool_name, bn_name, relu_name, add_bacth_norm=True, is_training=True,\n",
    "               out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), pool_size=(2,2), pool_strides=(2,2), keep_prob=0.5):\n",
    "    ### Encoder\n",
    "    # e.g. out_dim: 8; kern_size: (3,3); pool_size: (2,2); strides: (2,2);\n",
    "\n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    # filter:[filter_height, filter_width, in_channels, out_channels]\n",
    "    # input:[batch, in_height, in_width, in_channels]\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(inputs_, weight, stride, padding='SAME', name=conv_name), bias)\n",
    "    \n",
    "    # Now 28x28x\n",
    "    if add_bacth_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "        \n",
    "    # add activation function\n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    # add Max pooling\n",
    "    conv2d_maxpool = tf.nn.max_pool(conv2d, [1,pool_size[0],pool_size[1],1],\\\n",
    "                                    [1,pool_strides[0],pool_strides[1],1], padding='SAME', name=pool_name)\n",
    "                    \n",
    "    conv2d_maxpool = tf.layers.dropout(conv2d_maxpool, rate=keep_prob)\n",
    "\n",
    "    return conv2d_maxpool\n",
    "\n",
    "def deconv_layer(encoded, up_name, weight, bias, de_conv_name, bn_name, relu_name, add_batch_norm=True, is_training=True,\n",
    "                 up_size=[(2,2),(4,4),(5,5),(6,6)], out_dim=1, conv_kernsize=(2,2), conv_strides=(1,1), keep_prob=0.5):\n",
    "    ### Decoder\n",
    "    upsample = tf.image.resize_nearest_neighbor(encoded, up_size, name=up_name) # up_size: e.g.(7,7)\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    conv2d = tf.nn.bias_add(tf.nn.conv2d(upsample, weight, stride, padding='SAME', name=de_conv_name),\\\n",
    "                   bias)\n",
    "    \n",
    "    if add_batch_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "    \n",
    "    conv2d = tf.nn.relu(conv2d, name=relu_name)\n",
    "    \n",
    "    conv2d = tf.layers.dropout(conv2d, rate=keep_prob)\n",
    "    \n",
    "    return conv2d\n",
    "\n",
    "def output_layer(conv2d, weight, bias, logits_name, bn_name, decoded_name, add_batch_norm=True, is_training=True, \n",
    "                 out_dim=1, conv_kernsize=(3,3), conv_strides=(1,1)):\n",
    "    \n",
    "    # set strides of conv2d\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1]\n",
    "    \n",
    "    # conv2d\n",
    "    logits = tf.nn.bias_add(tf.nn.conv2d(conv2d, weight, stride, padding='SAME', name=logits_name),\\\n",
    "                   bias)\n",
    "    \n",
    "    if add_batch_norm:\n",
    "        conv2d = tf.layers.batch_normalization(conv2d, training=is_training, name=bn_name)\n",
    "    #Now 28x28x1\n",
    "    decoded = tf.nn.sigmoid(logits, name=decoded_name)\n",
    "    \n",
    "    return logits, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Au_1_bias_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-24eb2273c809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 64x64 -- 32x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     maxpool_1 = conv_layer(inputs, Au_1_weight_1, Au_1_bias_1, 'conv_1', 'pool_1', 'bn_1', 'relu_1', add_bacth_norm=False, \n\u001b[0m\u001b[1;32m     25\u001b[0m                            \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_kernsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkern_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_strides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv_strides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                            pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Au_1_bias_1' is not defined"
     ]
    }
   ],
   "source": [
    "conv_kern_size = (4,4)\n",
    "conv_strides = (1,1)\n",
    "pool_size = (2,2)\n",
    "pool_strides = (2,2)\n",
    "up_size = [(64,64)]\n",
    "de_kern_size = (4,4)\n",
    "de_conv_strides = (1,1)\n",
    "out_kern_size = (4,4)\n",
    "out_strides = (1,1)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "keep_p = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "training = tf.placeholder(tf.bool, name='training')\n",
    "inputs = tf.placeholder(tf.float32, [None, 64, 64, 1], name='inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 64, 64, 1], name='targets')\n",
    "\n",
    "# 64x64 -- 32x32\n",
    "with tf.name_scope('conv_layers'):\n",
    "    maxpool_1 = conv_layer(inputs, Au_1_weight_1, Au_1_bias_1, 'conv_1', 'pool_1', 'bn_1', 'relu_1', add_bacth_norm=False, \n",
    "                           is_training=training, out_dim=16, conv_kernsize=kern_size, conv_strides=conv_strides, \n",
    "                           pool_size=pool_size, pool_strides=pool_strides, keep_prob=keep_p)\n",
    "    \n",
    "    # Now 32x32x16 Hidden Layer 1 Au_1\n",
    "    conv_hidden_1 = tf.nn.bias_add(tf.nn.conv2d(maxpool_1, Au_1_weight_2, stride, padding='SAME', name='hidden_1'), Au_1_bias_2)\n",
    "    \n",
    "    # Now 32x32x8 Hidden Layer 2 Au_2\n",
    "    conv_hidden_2 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_1, Au_2_weight_2, stride, padding='SAME', name='hidden_1'), Au_2_bias_2)\n",
    "\n",
    "    # Now 32x32x8 Hidden Layer 3 Au_3\n",
    "    conv_hidden_3 = tf.nn.bias_add(tf.nn.conv2d(conv_hidden_2, Au_3_weight_2, stride, padding='SAME', name='hidden_1'), Au_3_bias_2)\n",
    "    \n",
    "    # Now 32x32x8 Deconv Layer\n",
    "    deconv_1 = deconv_layer(conv_hidden_3, up_name, Au_3_deconvweight_2, Au_3_debias_2, 'deconv_1', 'debn_1', 'derelu_1', add_batch_norm=True, is_training=training,\n",
    "                 up_size=up_size[0], out_dim=16, conv_kernsize=de_kern_size, conv_strides=de_conv_strides, keep_prob=keep_p)\n",
    "\n",
    "    # Now 64x64x16 Output Layer\n",
    "    logits, decoded = output_layer(conv2d, Au_3_outweight, Au_3_outbias, 'logits', 'outbn', 'output', add_batch_norm=True, is_training=training, \n",
    "                                   out_dim=1, conv_kernsize=out_kern_size, conv_strides=out_strides)\n",
    "    \n",
    "    # Now 64x64x1 Output\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits, name='loss')\n",
    "    cost = tf.reduce_mean(loss, name='cost')\n",
    "    opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "#     with tf.name_scope('saver'):\n",
    "#         saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(data, target, batch_size):\n",
    "    n_batches = len(data)//batch_size\n",
    "    data = data[:batch_size * n_batches]\n",
    "    target = target[:batch_size * n_batches]\n",
    "    for ii in range(0, batch_size*n_batches, batch_size):\n",
    "        data_batch = data[ii:ii + batch_size]\n",
    "        target_batch = target[ii:ii + batch_size]\n",
    "        \n",
    "        yield data_batch, target_batch\n",
    "        \n",
    "        \n",
    "def cal_accuracy(decoded, target):\n",
    "    error = abs(decoded - target)\n",
    "    Acc = (np.sum(error <= 0.08)/(decoded.shape[0]*decoded.shape[1]*decoded.shape[2]*decoded.shape[3]))*100\n",
    "    \n",
    "    return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "epochs = 1800\n",
    "batch_size = 80\n",
    "\n",
    "for lr in [0.001]:\n",
    "    save_string = './checkpoints_view_invariant/E2E_lr={}_bs={}.ckpt'.format(lr, batch_size)\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        iteration = 1\n",
    "        mean_val_loss = 0\n",
    "        mean_val_acc = 0\n",
    "        count_loss_not_decrease_epochs = 0\n",
    "        count_acc_not_decrease_epochs = 0\n",
    "        Last_val_loss = 0\n",
    "        Last_val_acc = 0\n",
    "\n",
    "\n",
    "        for e in range(epochs):\n",
    "            for train_batch, target_batch in get_batches(train_x, train_y, batch_size):\n",
    "\n",
    "                start = time.time()\n",
    "\n",
    "                feed_1 = {\n",
    "                            model.inputs_1: train_batch, \n",
    "                            model.targets_1: target_batch,\n",
    "                            model.keep_p: 0.6,\n",
    "                            model.learning_rate: lr,\n",
    "                            model.training:True\n",
    "                        }\n",
    "\n",
    "                train_loss, _, decoded_img = sess.run([model.cost, model.opt, model.decoded], feed_dict=feed_1)\n",
    "\n",
    "                train_acc = cal_accuracy(decoded_img, target_batch)\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    end = time.time()\n",
    "                    print(\"Epoch: {}/{},\".format(e+1, epochs),' '\n",
    "                              \"Iteration: {},\".format(iteration),' '\n",
    "                              \"Train loss: {:.3f},\".format(train_loss),'      '\n",
    "                              \"{:.1f}s /batch.\".format((end-start)/5),' '\n",
    "                              \"Train Accuracy: %{:.3f}\".format(train_acc))\n",
    "\n",
    "                    ##############################################################\n",
    "                    ######################## VALIDATION ##########################\n",
    "                    ##############################################################\n",
    "\n",
    "                if iteration%75==0:\n",
    "                    validation_loss = []\n",
    "                    validation_acc = []\n",
    "\n",
    "                    if batch_size >= len(val_x):\n",
    "                        val_batch_size = len(val_x)\n",
    "                    else: \n",
    "                        val_batch_size = batch_size\n",
    "\n",
    "\n",
    "                    for ii, (val_batch, val_target_batch) in enumerate(get_batches(val_x, val_y, val_batch_size)):\n",
    "                        feed_2 = {\n",
    "                                    model.inputs_1: val_batch,\n",
    "                                    model.targets_1: val_target_batch,\n",
    "                                    model.keep_p: 1,\n",
    "                                    model.training:True\n",
    "                                }\n",
    "\n",
    "                        val_loss, val_decoded_img = sess.run([model.cost, model.decoded], \n",
    "                                                                              feed_dict=feed_2)\n",
    "\n",
    "                        val_acc = cal_accuracy(val_decoded_img, val_target_batch)\n",
    "\n",
    "                        validation_loss.append(val_loss)\n",
    "                        validation_acc.append(val_acc)\n",
    "\n",
    "                    Last_val_loss = mean_val_loss\n",
    "                    Last_val_acc = mean_val_acc\n",
    "                    mean_val_loss = np.mean(np.array(validation_loss))\n",
    "                    mean_val_acc = np.mean(np.array(validation_acc))\n",
    "\n",
    "                    print()\n",
    "                    print(\"Validation loss: {:.3f},\".format(mean_val_loss),' '\n",
    "                              \"Validation accuracy: {:.3f},\".format(mean_val_acc))\n",
    "    #                           \"maxpool_3.shape: {}.\".format(max_pool_3[0].shape))\n",
    "                    print()\n",
    "                    ####### plot #######\n",
    "                if iteration%500==0:\n",
    "\n",
    "                    feed_3 = {\n",
    "                                model.inputs_1: val_x[:10],\n",
    "                                model.keep_p: 1,\n",
    "                                model.training:True\n",
    "                                }\n",
    "\n",
    "                    fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "                    in_imgs = val_y[:10]\n",
    "                    reconstructed = sess.run(model.decoded, feed_dict=feed_3)\n",
    "\n",
    "                        # plot out\n",
    "                    for images, row in zip([in_imgs, reconstructed], axes):\n",
    "                        for img, ax in zip(images, row):\n",
    "                            ax.imshow(img.reshape((64, 64)), cmap='Greys_r')\n",
    "                            ax.get_xaxis().set_visible(False)\n",
    "                            ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "                    fig.tight_layout(pad=0.1)\n",
    "\n",
    "                iteration += 1\n",
    "\n",
    "    #             # Early stopping  \n",
    "    #             if mean_val_Acc - Last_val_Acc <= -0.3:\n",
    "    #                 count_Acc_not_increase_epochs += 1\n",
    "    #             if Last_val_loss - mean_val_loss <= -0.01:\n",
    "    #                 count_loss_not_decrease_epochs += 1\n",
    "\n",
    "    #             if mean_val_Acc - Last_val_Acc <= -2:\n",
    "    #                 break\n",
    "    #             if count_Acc_not_increase_epochs >= 10:\n",
    "    #                 break\n",
    "    #             if count_loss_not_decrease_epochs >= 10:\n",
    "    #                 break\n",
    "\n",
    "        model.saver.save(sess, r\"{}\".format(save_string))\n",
    "        \n",
    "print(' ')\n",
    "print(' ')\n",
    "print(\"leraning_rate={},num_layers={},batch_size={} finished, saved\".format(lr, num_layers, batch_size))\n",
    "print(' ')\n",
    "print(' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
